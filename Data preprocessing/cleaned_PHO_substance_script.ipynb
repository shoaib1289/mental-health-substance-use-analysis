{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262296d-bb52-4f84-8f5d-9751d50d43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sheets detected ➜ Export\n",
      "Keeping only Monthly rows from each sheet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "FILE          = Path(\"C://Users//shoaib//Desktop//Mental_health_ed_project//PHO_Substance_ED_Hosp_Deaths_PHU_2014_2024.xlsx\")\n",
    "OUT_DIR       = Path(\"clean\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SUPPRESSION   = {\"–\", \"-\", \"—\", \"<5\", \"<6\", \"<10\",\n",
    "                 \"Suppressed\", \"NR\", \"N/A\", \"NA\", \"n/a\", \"na\"}\n",
    "\n",
    "NUMERIC_HINT  = [\n",
    "    \"number_of_cases\",\n",
    "    \"population\",\n",
    "    \"cases_per_100_000_population_annualized\",\n",
    "]\n",
    "\n",
    "KEEP_INTERVAL = \"Monthly\"     # ← change to \"Quarterly\" or \"Yearly\" if you prefer\n",
    "\n",
    "# ── 2. helper functions ────────────────────────────────────────\n",
    "def snake(text: str) -> str:\n",
    "    return re.sub(r\"[^0-9a-z]+\", \"_\", text.strip().lower()).strip(\"_\")\n",
    "\n",
    "\n",
    "def locate_header(raw_df: pd.DataFrame) -> int:\n",
    "    mask = raw_df.apply(\n",
    "        lambda r: r.astype(str)\n",
    "        .str.contains(\"public health unit\", case=False, na=False)\n",
    "        .any(),\n",
    "        axis=1,\n",
    "    )\n",
    "    return mask.idxmax()\n",
    "\n",
    "\n",
    "def clean_sheet(path: Path, sheet: str) -> pd.DataFrame:\n",
    "    \"\"\"Read → tidy → filter → return a truly clean DataFrame.\"\"\"\n",
    "    # 2.1 raw read (no header so nothing is coerced)\n",
    "    raw = pd.read_excel(path, sheet_name=sheet, header=None, dtype=str)\n",
    "\n",
    "    # 2.2 find real header row, re-read with proper header\n",
    "    hdr_row = locate_header(raw)\n",
    "    df = pd.read_excel(path, sheet_name=sheet, header=hdr_row)\n",
    "\n",
    "    # 2.3 drop empty rows / cols\n",
    "    df.dropna(how=\"all\", axis=0, inplace=True)\n",
    "    df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "\n",
    "    # 2.4 snake-case column names & strip footnote symbols\n",
    "    df.columns = [snake(c).replace(\"†\", \"\").replace(\"‡\", \"\") for c in df.columns]\n",
    "\n",
    "    # 2.5 suppression tokens \n",
    "    df.replace(list(SUPPRESSION), pd.NA, inplace=True)\n",
    "\n",
    "    # 2.6 remove banner / meta rows (indicator missing)\n",
    "    if \"indicator\" in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df[\"indicator\"].notna()].copy()\n",
    "        dropped = before - len(df)\n",
    "        if dropped:\n",
    "            print(f\"🔹  {sheet}: dropped {dropped} banner/meta rows\")\n",
    "\n",
    "    # 2.7 numeric casting\n",
    "    for col in NUMERIC_HINT:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # 2.8 date parsing\n",
    "    for col in (\"start_date_of_time_period\", \"end_date_of_time_period\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    # 2.9 keep a single aggregation level to avoid duplicate keys\n",
    "    if \"time_interval\" in df.columns:\n",
    "        df = df[df[\"time_interval\"].eq(KEEP_INTERVAL)].copy()\n",
    "\n",
    "    # 2.10 duplicate warning after all filters\n",
    "    key_cols = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"public_health_unit\",\n",
    "            \"indicator\",\n",
    "            \"start_date_of_time_period\",\n",
    "        ]\n",
    "        if c in df.columns\n",
    "    ]\n",
    "    if key_cols and df.duplicated(key_cols).any():\n",
    "        dup_cnt = df.duplicated(key_cols).sum()\n",
    "        print(f\"{sheet}: {dup_cnt} duplicate rows remain on {key_cols}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    assert FILE.exists(), f\"Workbook not found: {FILE.resolve()}\"\n",
    "    xls = pd.ExcelFile(FILE)\n",
    "    print(\" Sheets detected ➜\", \", \".join(xls.sheet_names))\n",
    "    print(\"Keeping only\", KEEP_INTERVAL, \"rows from each sheet\\n\")\n",
    "\n",
    "    parquet_ok = True\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "    except ImportError:\n",
    "        try:\n",
    "            import fastparquet  # noqa: F401\n",
    "        except ImportError:\n",
    "            parquet_ok = False\n",
    "            print(\"pyarrow / fastparquet not installed – Parquet export skipped\\n\")\n",
    "\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = clean_sheet(FILE, sheet)\n",
    "\n",
    "        stem = f\"{FILE.stem}_{snake(sheet)}_clean\"\n",
    "        csv_path = OUT_DIR / f\"{stem}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        if parquet_ok:\n",
    "            df.to_parquet(OUT_DIR / f\"{stem}.parquet\", index=False)\n",
    "\n",
    "        print(f\"{sheet:25s} ➜ {csv_path.name}   rows={len(df):,}\")\n",
    "\n",
    "    print(\"\\n All worksheets processed. Clean files live in:\", OUT_DIR.resolve())\n",
    "\n",
    "\n",
    "# ── 4. run when executed as a script ───────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30750bbd-76ba-435c-b37c-8630fe823145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
